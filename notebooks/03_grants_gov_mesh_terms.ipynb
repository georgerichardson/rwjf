{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Health Grant Descriptions with MeSH Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy\n",
    "import textacy\n",
    "import re\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import health terms.\n",
    "2. Filter terms down to the desired level.\n",
    "3. Add them as patterns to the matcher, labelled by the terms in the highest category level desired.\n",
    "4. Find matches in documents.\n",
    "5. Add a token attribute to hold the category label, and create a callback function to update this attribute. Run matching again.\n",
    "\n",
    "- Stretch goal: associate terms using part of speech in a network or coocurrence matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import Health Terms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/mesh_codes_processed_5_8_2018.json', 'r') as f:\n",
    "    health_terms = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Filter Terms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(string):\n",
    "    string = string.split(', ')\n",
    "    string = ' '.join(string[::-1])\n",
    "    return string.lower()\n",
    "\n",
    "def filter_terms(terms, order, on='ConceptStringProcessed'):\n",
    "    filtered = {}\n",
    "    for tree_number, properties in terms.items():\n",
    "        if properties['tree_order'] >= order:\n",
    "#             if properties.get('tree_{}_{}'.format(on, order)) is not None:\n",
    "            if order == 0:\n",
    "                top_parent = properties['tree_string_0']\n",
    "            else:\n",
    "                top_parent = properties['tree_{}_{}'.format(on, order)]\n",
    "            names = list(set([process_string(properties['{}'.format(t)]) for t in ['TermString', 'ConceptNameString', 'DescriptorNameString']]))\n",
    "#             names = list(set([process_string(properties['{}'.format(t)]) for t in ['ConceptNameString']]))\n",
    "            for name in names:\n",
    "                filtered[name.lower()] = top_parent\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'psychiatry and psychology'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "health_terms_0 = filter_terms(health_terms, 0)\n",
    "health_terms_0.pop('will')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/mesh_term_label_0_n.json', 'w') as f:\n",
    "    json.dump(health_terms_0, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_terms_filtered = filter_terms(health_terms, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55183"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(health_terms_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Import Project Descriptions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_grants = pd.read_csv('../data/processed/health_research_grants_4_26_2018.csv')\n",
    "\n",
    "n_docs = 200\n",
    "descriptions = health_grants.sample(n=200, random_state=42)['public_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 546 ms, sys: 3.61 ms, total: 550 ms\n",
      "Wall time: 551 ms\n"
     ]
    }
   ],
   "source": [
    "%time descriptions_clean = [textacy.preprocess_text(textacy.preprocess.fix_bad_unicode(d), lowercase=True) for d in descriptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Create PhraseMatcher**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.92 s, sys: 21.3 ms, total: 1.94 s\n",
      "Wall time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "%time docs = [tokenizer(d) for d in descriptions_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.82 s, sys: 183 ms, total: 6 s\n",
      "Wall time: 6.08 s\n"
     ]
    }
   ],
   "source": [
    "%time phrases = [(tokenizer(k), v) for k, v in health_terms_filtered.items()]\n",
    "\n",
    "for phrase, _ in phrases:\n",
    "    for token in phrase:\n",
    "        _ = tokenizer.vocab[token.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher_test = PhraseMatcher(tokenizer.vocab, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 115 ms, sys: 2.63 ms, total: 118 ms\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for phrase in phrases:\n",
    "    if phrase[1] is not None:\n",
    "        if len(phrase[0]) < 10:\n",
    "            matcher_test.add(phrase[1], None, phrase[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Find Matches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 4.76 ms, total: 1.88 s\n",
      "Wall time: 1.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "matches_test = []\n",
    "\n",
    "for text in descriptions_clean:\n",
    "    doc = tokenizer(text)\n",
    "#     for w in doc:\n",
    "#         _ = doc.vocab[w.text]\n",
    "    matches_test.append(matcher_test(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time to execute matching on all documents: 15.398800000000001 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimated time to execute matching on all documents:\", (0.6 + 2.4 + 1.8) * (len(health_grants) / n_docs / 60), 'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(It will take roughly half an hour to apply the current matching scheme to around 40,000 documents.)\n",
    "\n",
    "The results here seem pretty promising at first. I originally thought that there would be no matches in many documents, but it looks as if all documents have multiple matches. Perhaps the standardisation of language within the health and medical fields is pretty strong. One issue is words that are wrongly matched because they appear both in the MeSH terms and also in the documens, but clearly not in a medical sense. For example \"will\" is matched wherever it occurs as a \"psychological process\", which is clearly not going to be the case for many uses of the word.\n",
    "\n",
    "There is also an issue with a lot of terms not being picked up. For example \"healthcare\" is not picked up, as well as many terms consisting of multiple words, such as \"silicon sensor\", where only \"silicon\" is detected. Also key technologies, such as \"encryption\" are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension('is_mesh', default=False)\n",
    "Token.set_extension('mesh_label', default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ====================================================================================================\n",
      "14104734280646896271\tinorganic chemicals                             \tsilicon\n",
      "12114241926807388188\tpsychologic processes                           \twill\n",
      "10687308069696298868\ttissues                                         \ttissue\n",
      "12114241926807388188\tpsychologic processes                           \twill\n",
      "737360591271645834\thumanities                                      \tknowledge\n",
      "737360591271645834\thumanities                                      \thuman body\n",
      "4877011699741374108\thuman activities                                \ttravel\n",
      "10687308069696298868\ttissues                                         \ttissue\n",
      "737360591271645834\thumanities                                      \tknowledge\n",
      "14104734280646896271\tinorganic chemicals                             \tsilicon\n",
      "12114241926807388188\tpsychologic processes                           \twill\n",
      "3380507654813987851\tphysical processes                              \ttime\n",
      "10687308069696298868\ttissues                                         \ttissue\n",
      "14104734280646896271\tinorganic chemicals                             \tsilicon\n",
      "14104734280646896271\tinorganic chemicals                             \tsilicon\n",
      "8733477743570096877\tbody regions                                    \tback\n",
      "3380507654813987851\tphysical processes                              \ttime\n",
      "3380507654813987851\tphysical processes                              \ttime\n",
      "3380507654813987851\tphysical processes                              \ttime\n",
      "12114241926807388188\tpsychologic processes                           \tfeedback\n",
      "2456724971972959932\tequipment                                       \tequipment\n",
      "\n",
      "Full Text:\n",
      "a thin SILICON sensor WILL be integrated in a mechanical system for scanning through a TISSUE equivalent liquid phantom for providing extremely accurate information on the dose deposited by a hadron therapy beam as a function of depth into the phantom and on the position in the plane perpendicular to the beam axis . this WILL allow a precise KNOWLEDGE of the dose delivered to the patient ( 3d dose mapping ) with unprecedented spatial resolution and speed . the dose deposited in depth by a hadron therapy beam in the HUMAN BODY follows a typical distribution that terminates with the high energy release called bragg peak at the end of TRAVEL . this high energy deposition is exploited to maximise the effect to the cancer while giving limited amage to the surrounding helthy TISSUE . the accurate KNOWLEDGE of the energy deposited by the beam is key to planning the treatment on individual patients . also , the online monitoring of the bragg peak position stability during treatment is essential for the success of the treatment . \n",
      " thanks to the fast response of SILICON detectors , the bragg peak scan WILL provide real - TIME information to the operators on the dose to be released by the therapy beam in the target and surrounding TISSUE . this phantom makes use of a thinned SILICON detector to enhance the accuracy of the scan . the precision of the reconstructed profiles can be selected to be as fine as 10 microns ( this accuracy comes without cost or hardware complexity penalties ) . this instrument makes use of a single sensitive element ( the SILICON detector ) moved by a precision motor in axis with the therapy beam . the energy deposition of the beam is continuously measured by the sensor while moved from the entrance point of the phantom to the BACK end . the beam is stopped within the phantom volume and the scan delivers the energy deposition profile with great accuracy and in real TIME . this instrument would represent a significant improvement with respect to existing phantoms because of its high spatial resolution and real TIME response . the precision would prove very valuable at the planning stage of the treatment and the speed for real - TIME FEEDBACK . it could be used at any hadron therapy centre alone or in combination with other existing beam monitoring EQUIPMENT . \n",
      "\n",
      "100 ====================================================================================================\n",
      "5243318514652091556\tnon-medical public and private facilities       \tuniversity\n",
      "5243318514652091556\tnon-medical public and private facilities       \tuniversities\n",
      "10143216366574774699\tand agriculture industry technology             \ttechnology\n",
      "4508651938417329706\tphysical sciences                               \telectronic\n",
      "10143216366574774699\tand agriculture industry technology             \ttechnology\n",
      "10143216366574774699\tand agriculture industry technology             \tpatents\n",
      "1700477306715462547\tbehavior and behavior mechanisms                \ttrust\n",
      "7027329261096839145\tsocial sciences                                 \tcrime\n",
      "10143216366574774699\tand agriculture industry technology             \ttechnology\n",
      "14323146997540585630\thealth services administration                  \tdata security\n",
      "12114241926807388188\tpsychologic processes                           \twill\n",
      "12114241926807388188\tpsychologic processes                           \twill\n",
      "4877011699741374108\thuman activities                                \twork\n",
      "12114241926807388188\tpsychologic processes                           \twill\n",
      "10143216366574774699\tand agriculture industry technology             \ttechnology\n",
      "7563619441488496436\tand evaluation access health care quality       \tlicensing\n",
      "\n",
      "Full Text:\n",
      "metrarc is a UNIVERSITY spin out based in cambridge with research and development labs at the UNIVERSITIES of essex and kent , developing novel and ground - breaking TECHNOLOGY called icmetrics ™ for deriving secure encryption keys from the properties of digital systems without the need to store any of the encryption keys , digital signatures or icmetric data . metrarc is poised to revolutionise security for the internet - of - things ( iot ) and help safeguard the ELECTRONIC assets of business , and consumers through the commercialization of its icmetric platform , which is a disruptive TECHNOLOGY underpinned by £ 2 million of research funding ( from epsrc and eu ) and which is protected by a suite of ipr , including two granted us PATENTS . as the iot begins to burgeon the ability to establish TRUST and provence the identity and integrity of such devices becomes a paramount concern in order to ensure that user privacy and personal data are respected and cyber CRIME is prevented . one of the best ways to ensure data is not breached is by the use of encryption TECHNOLOGY . the market for security based on encryption is growing rapidly and predicted to be enormous by 2020 . one of the main problems with all current cencryption technologies is the need to store an encryption key . metrarc 's icmetric ™ techology does not store a key or any other data and is therfore of great interest to iot applications particularly where DATA SECURITY is paramount such as in healthcare applications . this project WILL investigate the proof - of - concept of employing novel secure system techniques for deriving encryption keys from the operating characteristics of devices comprising the iot focusing on dedicated sensor systems for sports and healthcare . this WILL build upon the highly significant results displayed by metrarc 's research WORK . the project deliverables WILL include two proof - of - concept demonstrators to showcase its disruptive TECHNOLOGY as suitable for LICENSING . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, match in enumerate(matches_test):\n",
    "    if i in [0, 100]:\n",
    "        print(i, '='*100)\n",
    "        for match_id, start, end in match:\n",
    "            string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "            span = docs[i][start:end]  # the matched span\n",
    "            print('{:<16}\\t{:<48}\\t{}'.format(match_id, string_id, span.text))\n",
    "            \n",
    "            tokens = [t for t in docs[i][start:end]]\n",
    "            for t in tokens:\n",
    "                t._.is_mesh = True\n",
    "                t._.mesh_label = health_terms_filtered.get(span.text)\n",
    "            doc_mod = []\n",
    "        for t in docs[i]:\n",
    "            if t._.is_mesh:\n",
    "                doc_mod.append(t.text.upper())\n",
    "            else:\n",
    "                doc_mod.append(t.text)\n",
    "        print('\\nFull Text:')\n",
    "        print(' '.join(doc_mod), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Add Token Attribute and Callback**\n",
    "\n",
    "(Leaving for now, as there are improvements to make first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuilding Docs using Matches Only\n",
    "\n",
    "Let's create versions of the documents that contain only the MeSH terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_descriptions = []\n",
    "for i, match in enumerate(matches_test):\n",
    "    d = []\n",
    "    for match_id, start, end in match:\n",
    "        string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "        span = docs[i][start:end]  # the matched span\n",
    "        d.append(span.text)\n",
    "    new_descriptions.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'silicon will tissue will knowledge human body travel tissue knowledge silicon will time tissue silicon silicon back time time time feedback equipment'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(new_descriptions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_term_counts = Counter(flatten(new_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6576"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flatten(new_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('will', 808),\n",
       " ('work', 139),\n",
       " ('disease', 109),\n",
       " ('award', 96),\n",
       " ('time', 80),\n",
       " ('risk', 74),\n",
       " ('technology', 63),\n",
       " ('methods', 62),\n",
       " ('resources', 60),\n",
       " ('hiv', 58),\n",
       " ('knowledge', 55),\n",
       " ('life', 54),\n",
       " ('population', 54),\n",
       " ('cells', 53),\n",
       " ('programs', 53),\n",
       " ('cell', 52),\n",
       " ('safety', 51),\n",
       " ('policy', 48),\n",
       " ('hospital', 45),\n",
       " ('education', 44)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_term_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply To All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_all = health_grants['public_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 1.63 s, total: 1min 38s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%time descriptions_all_clean = [textacy.preprocess_text(textacy.preprocess.fix_bad_unicode(d), lowercase=True) for d in descriptions_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 21s, sys: 3.76 s, total: 5min 24s\n",
      "Wall time: 5min 30s\n"
     ]
    }
   ],
   "source": [
    "%time docs = [tokenizer(d) for d in descriptions_all_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.63 s, sys: 237 ms, total: 4.86 s\n",
      "Wall time: 4.93 s\n"
     ]
    }
   ],
   "source": [
    "%time phrases = [(tokenizer(k), v) for k, v in health_terms_filtered.items()]\n",
    "\n",
    "for phrase, _ in phrases:\n",
    "    for token in phrase:\n",
    "        _ = tokenizer.vocab[token.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(tokenizer.vocab, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 117 ms, sys: 7 ms, total: 124 ms\n",
      "Wall time: 124 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for phrase in phrases:\n",
    "    if phrase[1] is not None:\n",
    "        if len(phrase[0]) < 10:\n",
    "            matcher.add(phrase[1], None, phrase[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 26s, sys: 2.21 s, total: 5min 28s\n",
      "Wall time: 5min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "matches = []\n",
    "\n",
    "for text in descriptions_all_clean:\n",
    "    doc = tokenizer(text)\n",
    "#     for w in doc:\n",
    "#         _ = doc.vocab[w.text]\n",
    "    matches.append(matcher(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_descriptions = []\n",
    "for i, match in enumerate(matches):\n",
    "    d = []\n",
    "    for match_id, start, end in match:\n",
    "        string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "        span = docs[i][start:end]  # the matched span\n",
    "        d.append(span.text)\n",
    "    new_descriptions.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_term_counts = Counter(flatten(new_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('will', 134092),\n",
       " ('disease', 18178),\n",
       " ('work', 16793),\n",
       " ('cells', 15420),\n",
       " ('award', 13002),\n",
       " ('risk', 12848),\n",
       " ('time', 12533),\n",
       " ('technology', 11314),\n",
       " ('methods', 11239),\n",
       " ('safety', 10701),\n",
       " ('cell', 10422),\n",
       " ('children', 9542),\n",
       " ('policy', 9538),\n",
       " ('programs', 9273),\n",
       " ('life', 9149),\n",
       " ('knowledge', 9094),\n",
       " ('population', 8695),\n",
       " ('hiv', 8678),\n",
       " ('brain', 7709),\n",
       " ('education', 7676)]"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_term_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We definitely need to get rid of the word \"will\" from all documents\" as it occurs 10 times more than for any other document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_descriptions_filtered = [[t for t in n_d if t != 'will'] for n_d in new_descriptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible Improvements**\n",
    "\n",
    "1. Enhance the list of terms by splitting into unigrams and filtering both the most uncommon and most common. Manually check for terms that are sufficiently appropriate in each category. - **Not sure if this is actually going to be useful after trying..**.\n",
    "2. ~~Use POS tagging to stop tagging words that are not used in a medical sense, e.g. \"will\". It seems like all of the MeSH terms are nouns, but I could be wrong...~~ **Doesn't work unless we use a regular `Matcher`, which is much slower. One alternative could be to filter terms by frequency instead.**\n",
    "3. Match on lemmas - **Also only possible with a regular `Matcher`.**\n",
    "\n",
    "**Think About**\n",
    "- Best way to use the transformed documents - clustering vs network.\n",
    "- Whether there's a computationally efficient way to find similar words within a document (probably most efficient just to stick with spaCy's PhraseMatcher).\n",
    "- Using POS tagging to stop tagging words that are not used in a medical sense, e.g. \"will\". It seems like all of the MeSH terms are nouns, but I could be wrong..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Improved Iteration\n",
    "\n",
    "### Create MeSH Term Unigrams\n",
    "\n",
    "There's a risk that many of the terms in the MeSh list may not be found in documents because the phrasing in the documents does not match the exact form of the MeSh term. To prevent this, we can split the terms into their unigram consituents, find the unigrams that occur under only one parent category, and do any other filtering to be left with a set of terms that are single words corresponding to a MeSH category at the order we have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_phrases = [(a.text, b) for a, b in phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('calcium channel agonists', 'and proteins peptides amino acids'),\n",
       " ('(+)-isomer bay-k-8644', 'heterocyclic compounds'),\n",
       " ('(+)-isomer methoxyhydroxyphenylglycol', 'organic chemicals'),\n",
       " ('methoxyhydroxyphenylglycol', 'organic chemicals'),\n",
       " ('muscarinic antagonists', 'heterocyclic compounds'),\n",
       " ('(+)-isomer oxyphenonium bromide', 'organic chemicals'),\n",
       " ('serotonin receptor agonists', 'and proteins peptides amino acids'),\n",
       " ('(+,-)-isomer 2,5-dimethoxy-4-methylamphetamine', 'organic chemicals'),\n",
       " ('cytochrome p-450 cyp2d6 inhibitors', 'heterocyclic compounds'),\n",
       " ('(+-)-isomer bupropion', 'organic chemicals')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_phrases[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline of the process:\n",
    "\n",
    "- Map unigrams to labels\n",
    "- Eliminate those that appear in multiple labels\n",
    "- Eliminate those that are too uncommon\n",
    "- Eliminate those that are too common\n",
    "- Eliminate from map \n",
    "- Update original map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we split the phrases into unigrams and map them to their labels (`unigram_mesh_terms`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_mesh_terms = []\n",
    "\n",
    "for phrase, label in string_phrases:\n",
    "    phrase = phrase.split(' ')\n",
    "    for p in phrase:\n",
    "        unigram_mesh_terms.append((p, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then find out how many categories each of the unigrams appears in (`unigram_mesh_terms_label_counts`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_mesh_terms_label_counts = {}\n",
    "\n",
    "for phrase, label in unigram_mesh_terms:\n",
    "    if phrase not in unigram_mesh_terms_label_counts:\n",
    "        unigram_mesh_terms_label_counts[phrase] = {'terms': [],\n",
    "                                                  'count': 0}\n",
    "    if label in unigram_mesh_terms_label_counts[phrase]['terms']:\n",
    "        continue\n",
    "    else:\n",
    "        unigram_mesh_terms_label_counts[phrase]['terms'].append(label)\n",
    "        unigram_mesh_terms_label_counts[phrase]['count'] = len(unigram_mesh_terms_label_counts[phrase]['terms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we find all of those that appear in more than one category, and we remove them from the original list of unigrams (`unigram_mesh_term_singles`), and make a `Counter` of the terms (`unigram_mesh_term_counts`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_mesh_terms_label_multis = [phrase for phrase, properties in unigram_mesh_terms_label_counts.items() if properties['count'] != 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_mesh_term_singles = [(t[0], t[1]) for t in unigram_mesh_terms if t[0] not in unigram_mesh_terms_label_multis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39079"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_mesh_term_singles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_mesh_term_counts = Counter([t[0] for t in unigram_mesh_term_singles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still left with 30,000 unigrams, so we probably need to filter a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30083"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_mesh_term_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some of the most common..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transporter', 73),\n",
       " ('chromosomes', 61),\n",
       " ('republic', 43),\n",
       " ('rats', 42),\n",
       " ('institute', 27),\n",
       " ('electrophoresis', 26),\n",
       " ('oxidoreductases', 26),\n",
       " ('cyclin', 26),\n",
       " ('assays', 25),\n",
       " ('transporters', 24),\n",
       " ('lactobacillus', 24),\n",
       " ('isotopes', 23),\n",
       " ('waste', 22),\n",
       " ('spectrometry', 22),\n",
       " ('myosin', 21),\n",
       " ('gel', 20),\n",
       " ('acetyltransferase', 20),\n",
       " ('inborn', 20),\n",
       " ('perceptions', 18),\n",
       " ('methyltransferase', 18)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_mesh_term_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the most uncommon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon_n = 1\n",
    "unigram_mesh_term_counts_uncommon = Counter({k: v for k, v in unigram_mesh_term_counts.items() if v <= uncommon_n})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bay-k-8644', 1),\n",
       " ('oxyphenonium', 1),\n",
       " ('(+,-)-isomer', 1),\n",
       " ('2,5-dimethoxy-4-methylamphetamine', 1),\n",
       " ('bupropion', 1),\n",
       " ('fenfluramine', 1),\n",
       " ('tocainide', 1),\n",
       " ('tomoxetine', 1),\n",
       " ('bisoprolol', 1),\n",
       " ('encainide', 1),\n",
       " ('(1', 1),\n",
       " ('alpha,2', 1),\n",
       " ('(1(s*(s*)),2', 1),\n",
       " ('fosinopril', 1),\n",
       " ('antihypertensive', 1),\n",
       " ('(11', 1),\n",
       " ('(11a', 1),\n",
       " ('anthramycin', 1),\n",
       " ('(13', 1),\n",
       " ('radiopharmaceuticals', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_mesh_term_counts_uncommon.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that we can filter:\n",
    "- Terms that are only numbers or numbers and punctuation\n",
    "- Short tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_mesh_term_singles = [t for t in unigram_mesh_term_singles if not t[0].isdigit()]\n",
    "unigram_mesh_term_singles = [t for t in unigram_mesh_term_singles if re.match(\"^[A-Za-z0-9]*$\", t[0])]\n",
    "unigram_mesh_term_singles = [t for t in unigram_mesh_term_singles if len(t[0]) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_mesh_term_singles_counts = Counter(unigram_mesh_term_singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing this, we can see that we're left with a more sensible list. There's a few terms that seem a bit vague, but without handpicking them out, it seems like it could be an unnecessarily complex task to remove them, so we'll leave it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('transporter', 'and proteins peptides amino acids'), 73),\n",
       " (('chromosomes', 'genetic processes'), 61),\n",
       " (('republic', 'geographic locations'), 43),\n",
       " (('rats', 'eukaryota'), 42),\n",
       " (('institute', 'health care economics and organizations'), 27),\n",
       " (('electrophoresis', 'investigative techniques'), 26),\n",
       " (('oxidoreductases', 'enzymes and coenzymes'), 26),\n",
       " (('cyclin', 'and proteins peptides amino acids'), 26),\n",
       " (('assays', 'investigative techniques'), 25),\n",
       " (('transporters', 'and proteins peptides amino acids'), 24),\n",
       " (('lactobacillus', 'bacteria'), 24),\n",
       " (('isotopes', 'inorganic chemicals'), 23),\n",
       " (('waste', 'environment and public health'), 22),\n",
       " (('spectrometry', 'investigative techniques'), 22),\n",
       " (('myosin', 'and proteins peptides amino acids'), 21),\n",
       " (('gel', 'investigative techniques'), 20),\n",
       " (('acetyltransferase', 'enzymes and coenzymes'), 20),\n",
       " (('inborn', 'nutritional and metabolic diseases'), 20),\n",
       " (('perceptions', 'psychologic processes'), 18),\n",
       " (('methyltransferase', 'enzymes and coenzymes'), 18)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_mesh_term_singles_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon_n = 1\n",
    "unigram_mesh_term_sinlges_counts_uncommon = Counter({k: v for k, v in unigram_mesh_term_singles_counts.items() if v <= uncommon_n})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('oxyphenonium', 'organic chemicals'), 1),\n",
       " (('bupropion', 'organic chemicals'), 1),\n",
       " (('fenfluramine', 'organic chemicals'), 1),\n",
       " (('tocainide', 'organic chemicals'), 1),\n",
       " (('tomoxetine', 'organic chemicals'), 1),\n",
       " (('bisoprolol', 'organic chemicals'), 1),\n",
       " (('encainide', 'organic chemicals'), 1),\n",
       " (('fosinopril', 'and proteins peptides amino acids'), 1),\n",
       " (('antihypertensive', 'heterocyclic compounds'), 1),\n",
       " (('anthramycin', 'heterocyclic compounds'), 1),\n",
       " (('radiopharmaceuticals', 'organic chemicals'), 1),\n",
       " (('ethynodiol', 'polycyclic compounds'), 1),\n",
       " (('menthol', 'lipids'), 1),\n",
       " (('antipruritics', 'signs and symptoms pathological conditions'), 1),\n",
       " (('enprostil', 'biological factors'), 1),\n",
       " (('permethrin', 'organic chemicals'), 1),\n",
       " (('swainsonine', 'heterocyclic compounds'), 1),\n",
       " (('lincomycin', 'carbohydrates'), 1),\n",
       " (('pipecuronium', 'heterocyclic compounds'), 1),\n",
       " (('pregnanolone', 'polycyclic compounds'), 1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_mesh_term_sinlges_counts_uncommon.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_mesh_terms = list(set(unigram_mesh_term_singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25617"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_mesh_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Create PhraseMatcher**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_mesh_terms_parsed = [(tokenizer(k), v) for k, v in unigram_mesh_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.07 ms, sys: 64 µs, total: 1.13 ms\n",
      "Wall time: 1.14 ms\n"
     ]
    }
   ],
   "source": [
    "%time phrases_all = phrases + unigram_mesh_terms_parsed\n",
    "\n",
    "for phrase, _ in phrases_all:\n",
    "    for token in phrase:\n",
    "        _ = tokenizer.vocab[token.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher_test_all = PhraseMatcher(tokenizer.vocab, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 168 ms, sys: 6.97 ms, total: 175 ms\n",
      "Wall time: 175 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for phrase in phrases_all:\n",
    "    if phrase[1] is not None:\n",
    "        if len(phrase[0]) < 10:\n",
    "            matcher_test_all.add(phrase[1], None, phrase[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Find Matches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.9 s, sys: 18.9 ms, total: 1.92 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "matches_test_all = []\n",
    "\n",
    "for text in descriptions_clean:\n",
    "    doc = tokenizer(text)\n",
    "#     for w in doc:\n",
    "#         _ = doc.vocab[w.text]\n",
    "    matches_test_all.append(matcher_test(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time to execute matching on all documents: 15.398800000000001 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimated time to execute matching on all documents:\", (0.6 + 2.4 + 1.8) * (len(health_grants) / n_docs / 60), 'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(It will take roughly half an hour to apply the current matching scheme to around 40,000 documents.)\n",
    "\n",
    "The results here seem pretty promising at first. I originally thought that there would be no matches in many documents, but it looks as if all documents have multiple matches. Perhaps the standardisation of language within the health and medical fields is pretty strong. One issue is words that are wrongly matched because they appear both in the MeSH terms and also in the documens, but clearly not in a medical sense. For example \"will\" is matched wherever it occurs as a \"psychological process\", which is clearly not going to be the case for many uses of the word.\n",
    "\n",
    "There is also an issue with a lot of terms not being picked up. For example \"healthcare\" is not picked up, as well as many terms consisting of multiple words, such as \"silicon sensor\", where only \"silicon\" is detected. Also key technologies, such as \"encryption\" are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ====================================================================================================\n",
      "14104734280646896271\tinorganic chemicals                             \tsilicon\n",
      "12114241926807388188\tpsychologic processes                           \twill\n",
      "10687308069696298868\ttissues                                         \ttissue\n",
      "12114241926807388188\tpsychologic processes                           \twill\n",
      "737360591271645834\thumanities                                      \tknowledge\n",
      "737360591271645834\thumanities                                      \thuman body\n",
      "4877011699741374108\thuman activities                                \ttravel\n",
      "10687308069696298868\ttissues                                         \ttissue\n",
      "737360591271645834\thumanities                                      \tknowledge\n",
      "14104734280646896271\tinorganic chemicals                             \tsilicon\n",
      "12114241926807388188\tpsychologic processes                           \twill\n",
      "3380507654813987851\tphysical processes                              \ttime\n",
      "10687308069696298868\ttissues                                         \ttissue\n",
      "14104734280646896271\tinorganic chemicals                             \tsilicon\n",
      "14104734280646896271\tinorganic chemicals                             \tsilicon\n",
      "8733477743570096877\tbody regions                                    \tback\n",
      "3380507654813987851\tphysical processes                              \ttime\n",
      "3380507654813987851\tphysical processes                              \ttime\n",
      "3380507654813987851\tphysical processes                              \ttime\n",
      "12114241926807388188\tpsychologic processes                           \tfeedback\n",
      "2456724971972959932\tequipment                                       \tequipment\n",
      "\n",
      "Full Text:\n",
      "a thin SILICON sensor WILL be integrated in a mechanical system for scanning through a TISSUE equivalent liquid phantom for providing extremely accurate information on the dose deposited by a hadron therapy beam as a function of depth into the phantom and on the position in the plane perpendicular to the beam axis . this WILL allow a precise KNOWLEDGE of the dose delivered to the patient ( 3d dose mapping ) with unprecedented spatial resolution and speed . the dose deposited in depth by a hadron therapy beam in the HUMAN BODY follows a typical distribution that terminates with the high energy release called bragg peak at the end of TRAVEL . this high energy deposition is exploited to maximise the effect to the cancer while giving limited amage to the surrounding helthy TISSUE . the accurate KNOWLEDGE of the energy deposited by the beam is key to planning the treatment on individual patients . also , the online monitoring of the bragg peak position stability during treatment is essential for the success of the treatment . \n",
      " thanks to the fast response of SILICON detectors , the bragg peak scan WILL provide real - TIME information to the operators on the dose to be released by the therapy beam in the target and surrounding TISSUE . this phantom makes use of a thinned SILICON detector to enhance the accuracy of the scan . the precision of the reconstructed profiles can be selected to be as fine as 10 microns ( this accuracy comes without cost or hardware complexity penalties ) . this instrument makes use of a single sensitive element ( the SILICON detector ) moved by a precision motor in axis with the therapy beam . the energy deposition of the beam is continuously measured by the sensor while moved from the entrance point of the phantom to the BACK end . the beam is stopped within the phantom volume and the scan delivers the energy deposition profile with great accuracy and in real TIME . this instrument would represent a significant improvement with respect to existing phantoms because of its high spatial resolution and real TIME response . the precision would prove very valuable at the planning stage of the treatment and the speed for real - TIME FEEDBACK . it could be used at any hadron therapy centre alone or in combination with other existing beam monitoring EQUIPMENT . \n",
      "\n",
      "100 ====================================================================================================\n",
      "5243318514652091556\tnon-medical public and private facilities       \tuniversity\n",
      "5243318514652091556\tnon-medical public and private facilities       \tuniversities\n",
      "10143216366574774699\tand agriculture industry technology             \ttechnology\n",
      "4508651938417329706\tphysical sciences                               \telectronic\n",
      "10143216366574774699\tand agriculture industry technology             \ttechnology\n",
      "10143216366574774699\tand agriculture industry technology             \tpatents\n",
      "1700477306715462547\tbehavior and behavior mechanisms                \ttrust\n",
      "7027329261096839145\tsocial sciences                                 \tcrime\n",
      "10143216366574774699\tand agriculture industry technology             \ttechnology\n",
      "14323146997540585630\thealth services administration                  \tdata security\n",
      "12114241926807388188\tpsychologic processes                           \twill\n",
      "12114241926807388188\tpsychologic processes                           \twill\n",
      "4877011699741374108\thuman activities                                \twork\n",
      "12114241926807388188\tpsychologic processes                           \twill\n",
      "10143216366574774699\tand agriculture industry technology             \ttechnology\n",
      "7563619441488496436\tand evaluation access health care quality       \tlicensing\n",
      "\n",
      "Full Text:\n",
      "metrarc is a UNIVERSITY spin out based in cambridge with research and development labs at the UNIVERSITIES of essex and kent , developing novel and ground - breaking TECHNOLOGY called icmetrics ™ for deriving secure encryption keys from the properties of digital systems without the need to store any of the encryption keys , digital signatures or icmetric data . metrarc is poised to revolutionise security for the internet - of - things ( iot ) and help safeguard the ELECTRONIC assets of business , and consumers through the commercialization of its icmetric platform , which is a disruptive TECHNOLOGY underpinned by £ 2 million of research funding ( from epsrc and eu ) and which is protected by a suite of ipr , including two granted us PATENTS . as the iot begins to burgeon the ability to establish TRUST and provence the identity and integrity of such devices becomes a paramount concern in order to ensure that user privacy and personal data are respected and cyber CRIME is prevented . one of the best ways to ensure data is not breached is by the use of encryption TECHNOLOGY . the market for security based on encryption is growing rapidly and predicted to be enormous by 2020 . one of the main problems with all current cencryption technologies is the need to store an encryption key . metrarc 's icmetric ™ techology does not store a key or any other data and is therfore of great interest to iot applications particularly where DATA SECURITY is paramount such as in healthcare applications . this project WILL investigate the proof - of - concept of employing novel secure system techniques for deriving encryption keys from the operating characteristics of devices comprising the iot focusing on dedicated sensor systems for sports and healthcare . this WILL build upon the highly significant results displayed by metrarc 's research WORK . the project deliverables WILL include two proof - of - concept demonstrators to showcase its disruptive TECHNOLOGY as suitable for LICENSING . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, match in enumerate(matches_test):\n",
    "    if i in [0, 100]:\n",
    "        print(i, '='*100)\n",
    "        for match_id, start, end in match:\n",
    "            string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "            span = docs[i][start:end]  # the matched span\n",
    "            print('{:<16}\\t{:<48}\\t{}'.format(match_id, string_id, span.text))\n",
    "            \n",
    "            tokens = [t for t in docs[i][start:end]]\n",
    "            for t in tokens:\n",
    "                t._.is_mesh = True\n",
    "                t._.mesh_label = health_terms_filtered.get(span.text)\n",
    "            doc_mod = []\n",
    "        for t in docs[i]:\n",
    "            if t._.is_mesh:\n",
    "                doc_mod.append(t.text.upper())\n",
    "            else:\n",
    "                doc_mod.append(t.text)\n",
    "        print('\\nFull Text:')\n",
    "        print(' '.join(doc_mod), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_terms_all = {k.text: v for k, v in phrases_all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/mesh_term_label_1_n.json', 'w') as f:\n",
    "    json.dump(health_terms_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy Custom Pipeline Module for Phrase Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension('is_mesh', default=False)\n",
    "Token.set_extension('mesh_label', default=None)\n",
    "Doc.set_extension('mesh_terms', default=[])\n",
    "Doc.set_extension('mesh_label', default=[])\n",
    "\n",
    "class MeshTermMatcher(object):\n",
    "    name = 'mesh_term_matcher'\n",
    "\n",
    "    def __init__(self, tokenizer, terms):\n",
    "        patterns = [(tokenizer(text), label) for text, label in terms]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        for pattern, label in patterns:\n",
    "            self.matcher.add(label, None, pattern)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc._.mesh_labels = terms.get(label)\n",
    "            for token in span:\n",
    "                label = terms.get(token.text)\n",
    "                token._.is_mesh = True\n",
    "                token._.mesh_label = label\n",
    "                doc._.mesh_labels.append(label)\n",
    "                doc._.mesh_terms.append(token.text)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with MeSH Term Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_terms(terms, min_order, max_order, on='ConceptStringProcessed', get_strings_from=['TermString', 'ConceptNameString', 'DescriptorNameString']):\n",
    "    filtered = {}\n",
    "    for tree_number, properties in terms.items():\n",
    "        if (properties['tree_order'] >= min_order) & (properties['tree_order'] <= max_order):\n",
    "            top_parent = properties['tree_{}_{}'.format(on, min_order)]\n",
    "            names = list(set([process_string(properties['{}'.format(t)]) for t in get_strings_from]))\n",
    "            for name in names:\n",
    "                filtered[name.lower()] = top_parent\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_terms_filtered_1_3 = filter_terms(health_terms, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12986"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(health_terms_filtered_1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
