{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "import gensim\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "\n",
    "import spacy\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_grants_df = pd.read_csv('../data/health_research_grants_2018_04_23_1436.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_name</th>\n",
       "      <th>lad13nm</th>\n",
       "      <th>participant_name</th>\n",
       "      <th>project_start_date</th>\n",
       "      <th>project_title</th>\n",
       "      <th>public_description</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>project_id</th>\n",
       "      <th>paragraph_vectors</th>\n",
       "      <th>year</th>\n",
       "      <th>grants_funding</th>\n",
       "      <th>project_start_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Health Resources and Services Administration</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr 03, 2013</td>\n",
       "      <td>Ryan White HIV/AIDS Program Part D Grants for ...</td>\n",
       "      <td>This announcement solicits applications for fi...</td>\n",
       "      <td>grants_gov</td>\n",
       "      <td>HRSA-13-264</td>\n",
       "      <td>[-0.35462067 -0.41949466 -0.27364257 -0.659733...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Health Resources and Services Administration</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jan 30, 2012</td>\n",
       "      <td>Ryan White HIV/AIDS Program Part D Grants for ...</td>\n",
       "      <td>The purpose of this funding opportunity announ...</td>\n",
       "      <td>grants_gov</td>\n",
       "      <td>HRSA-12-073</td>\n",
       "      <td>[-0.58284324 -0.42862821 -0.52920008 -0.820197...</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>70000000.0</td>\n",
       "      <td>2012-01-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    agency_name lad13nm participant_name  \\\n",
       "0  Health Resources and Services Administration     NaN              NaN   \n",
       "1  Health Resources and Services Administration     NaN              NaN   \n",
       "\n",
       "  project_start_date                                      project_title  \\\n",
       "0       Apr 03, 2013  Ryan White HIV/AIDS Program Part D Grants for ...   \n",
       "1       Jan 30, 2012  Ryan White HIV/AIDS Program Part D Grants for ...   \n",
       "\n",
       "                                  public_description  dataset_id   project_id  \\\n",
       "0  This announcement solicits applications for fi...  grants_gov  HRSA-13-264   \n",
       "1  The purpose of this funding opportunity announ...  grants_gov  HRSA-12-073   \n",
       "\n",
       "                                   paragraph_vectors    year  grants_funding  \\\n",
       "0  [-0.35462067 -0.41949466 -0.27364257 -0.659733...  2013.0             NaN   \n",
       "1  [-0.58284324 -0.42862821 -0.52920008 -0.820197...  2012.0      70000000.0   \n",
       "\n",
       "  project_start_datetime  \n",
       "0             2013-04-03  \n",
       "1             2012-01-30  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "health_grants_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_text(text):\n",
    "    # remove newline characters and lower case\n",
    "    text = text.replace('\\n', '').replace('\\r', '').strip().lower()\n",
    "    # replace URLs with single identifier\n",
    "    text = re.sub(r\"[\\(]?http\\S+\", 'URL', text)\n",
    "    text = re.sub(r\"[\\(]?www\\S+\", 'URL', text)\n",
    "    # replace digits with #\n",
    "    #     text = re.sub(r\"\\d\", '#', text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = health_grants_df['public_description'].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two corpora - one for exploration and one for topic modelling**\n",
    "- Remove stop words\n",
    "- Remove high and low frequency words\n",
    "- Remove punctuation\n",
    "\n",
    "**Exploration**\n",
    "- Remove all digits\n",
    "\n",
    "**Topic Modelling**\n",
    "- Part of speech tagging\n",
    "- Remove punctuation\n",
    "- Transform into sub-corpora of - tokens, lemmas, part of speech\n",
    "\n",
    "**For both after preprocessing**\n",
    "- Generate ngram transformed corpus\n",
    "- Generate phrase transformed corpus\n",
    "- Generate combined corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parsed_corpus(docs, directory, prefix, length, suffix='', top_n=None):\n",
    "    \n",
    "    order = len(str(length)) + 1\n",
    "    # create generic path\n",
    "    if directory[-1] == os.sep:\n",
    "        generic_path = directory + prefix + '_{}' + suffix\n",
    "    else:\n",
    "        generic_path = directory + os.sep + prefix + '_{}' + suffix\n",
    "        \n",
    "    for i, doc in enumerate(docs):\n",
    "        n = str(i).rjust(order, '0')        \n",
    "        doc.to_disk(generic_path.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parsed_corpus(directory, prefix, length, model, suffix='', top_n=None):\n",
    "    docs = []\n",
    "    order = len(str(length)) + 1\n",
    "    # create generic path\n",
    "    if directory[-1] == os.sep:\n",
    "        generic_path = directory + prefix + '_{}' + suffix\n",
    "    else:\n",
    "        generic_path = directory + os.sep + prefix + '_{}' + suffix\n",
    "        \n",
    "    for i in range(length):\n",
    "        if i < top_n:\n",
    "            n = str(i).rjust(order, '0')        \n",
    "            doc = Doc(model.vocab).from_disk(generic_path.format(n))\n",
    "            docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyParser(TransformerMixin):\n",
    "    def __init__(self, spacy_model, **pipe_kwargs):\n",
    "        self.spacy_model = spacy_model\n",
    "        self.pipe_kwargs = pipe_kwargs\n",
    "        \n",
    "    def fit(self, texts, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts, *args):\n",
    "        pipe_kwargs = self.pipe_kwargs\n",
    "        spacy_model = self.spacy_model\n",
    "        docs = [spacy_model(text) for text in texts]\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SpacyParser(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = parser.transform(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(TransformerMixin):\n",
    "    def __init__(self, **tokenize_kwargs):\n",
    "        self.tokenize_kwargs = tokenize_kwargs\n",
    "        \n",
    "    def tokenize(self, texts):\n",
    "        tokens = []\n",
    "        tokenize_kwargs = self.tokenize_kwargs\n",
    "        for text in texts:\n",
    "            # tokenize each message; simply lowercase & match alphabetic chars, for now\n",
    "            # yield gensim.utils.tokenize(text, **tokenize_kwargs)\n",
    "            tokens.append(list(gensim.utils.tokenize(text, **tokenize_kwargs)))\n",
    "        return tokens\n",
    "                \n",
    "    def fit(self, texts, *args):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts, *args):\n",
    "        return self.tokenize(texts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer(TransformerMixin):\n",
    "    def __init__(self, **lemmatize_kwargs):\n",
    "        self.lemmatize_kwargs = lemmatize_kwargs\n",
    "        \n",
    "    def lemmatize(self, texts):\n",
    "        lemmatize_kwargs = self.lemmatize_kwargs\n",
    "        lemmas = []\n",
    "        for text in texts:\n",
    "            # tokenize each message; simply lowercase & match alphabetic chars, for now\n",
    "            # yield gensim.utils.lemmatize(text, **lemmatize_kwargs)\n",
    "            text = gensim.utils.lemmatize(text, **lemmatize_kwargs)\n",
    "            lemmas.append(list(text))\n",
    "                \n",
    "    def fit(self, texts, *args):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts, *args):\n",
    "        return self.lemmatize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [['the', 'mayor', 'of', 'new', 'york'],\n",
    "         ['going', 'to', 'new', 'york', 'tomorrow'],\n",
    "         ['i', 'love', 'new', 'york'],\n",
    "         ['new', 'york', 'is', 'the', 'best'],\n",
    "         ['the', 'pizza', 'in', 'new', 'york', 'is', 'ok'],\n",
    "         ['the', 'mayor', 'of', 'new', 'york', 'is', 'rich']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean > tokenize > ngrammer > stopword removal > (lemmatize) >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, lower=True, remove=['SYM', 'PUNCT']):\n",
    "        self.lower = lower\n",
    "        self.remove = remove\n",
    "        \n",
    "    def fit(self, docs, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, docs, *args):\n",
    "        lower = self.lower\n",
    "        remove = self.remove\n",
    "        tokenized = []\n",
    "        for d in docs:\n",
    "            tokens = []\n",
    "            for token in d:\n",
    "                if token.pos_ in remove:\n",
    "                    continue\n",
    "                else:\n",
    "                    if lower:\n",
    "                        tokens.append(token.lower_)\n",
    "                    else:\n",
    "                        tokens.append(token.text)\n",
    "            tokenized.append(tokens)\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(remove=['SYM', 'PUNCT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tk.transform(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nlp('I will be Playing (iN) the Park with a firing range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, remove=['SYM', 'PUNCT']):\n",
    "        self.remove = remove\n",
    "        \n",
    "    def fit(self, docs, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, docs, *args):\n",
    "        remove = self.remove\n",
    "        lemmatized = []\n",
    "        for d in docs:\n",
    "            lemmas = []\n",
    "            for token in d:\n",
    "                if token.pos_ in remove:\n",
    "                    continue\n",
    "                else:\n",
    "                    lemmas.append(token.lemma_)\n",
    "            lemmatized.append(lemmas)\n",
    "        return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = lm.transform(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGrammer(TransformerMixin):\n",
    "    def __init__(self, n=3, **phrase_kwargs):\n",
    "        self.n = n\n",
    "        self.phrase_kwargs = phrase_kwargs\n",
    "        \n",
    "    def fit(self, texts, *args):\n",
    "        n = self.n\n",
    "        if n > 1:\n",
    "            for _ in range(n - 1):\n",
    "                ngrams = Phrases(texts)\n",
    "                ngrammer = Phraser(ngrams)\n",
    "                texts = ngrammer[texts]\n",
    "            self.ngrams = ngrams\n",
    "            self.ngrammer = ngrammer\n",
    "        return self\n",
    "        \n",
    "    def ngram(self, texts):\n",
    "        phrase_kwargs = self.phrase_kwargs\n",
    "        n = self.n\n",
    "        if n > 1:\n",
    "            for _ in range(n - 1):\n",
    "                ngrams = Phrases(texts, **phrase_kwargs)\n",
    "                ngrammer = Phraser(ngrams)\n",
    "                texts = ngrammer[texts]\n",
    "            self.ngrams = ngrams\n",
    "            self.ngrammer = ngrammer\n",
    "        return list(texts)\n",
    "        \n",
    "    def transform(self, texts, *args):\n",
    "        return self.ngram(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopWordRemover(TransformerMixin):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = NGrammer(**{'threshold': 5, 'min_count': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO : collected 5604 word types from a corpus of 7774 words (unigram + bigrams) and 10 sentences\n",
      "INFO : using 5604 counts as vocab in Phrases<0 vocab, min_count=10, threshold=5, max_vocab_size=40000000>\n",
      "INFO : source_vocab length 5604\n",
      "INFO : Phraser built with 17 17 phrasegrams\n",
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO : collected 5664 word types from a corpus of 7447 words (unigram + bigrams) and 10 sentences\n",
      "INFO : using 5664 counts as vocab in Phrases<0 vocab, min_count=10, threshold=5, max_vocab_size=40000000>\n",
      "INFO : source_vocab length 5664\n",
      "INFO : Phraser built with 18 18 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "ngrammed = ng.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CleanText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-65fc00d6478b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCleanText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'lower'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGrammer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CleanText' is not defined"
     ]
    }
   ],
   "source": [
    "ct = CleanText(log_every=5000)\n",
    "processed = ct.transform(descriptions)\n",
    "tk = Tokenizer(**{'lower': True})\n",
    "tokenized = tk.transform(processed)\n",
    "ng = NGrammer(n=2)\n",
    "ngrams = ng.transform(tokenized)\n",
    "# lm = Lemmatizer()\n",
    "# lemmas = lm.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'mayor', 'of', 'new_york'], ['going', 'to', 'new_york', 'tomorrow'], ['i', 'love', 'new_york']]\n"
     ]
    }
   ],
   "source": [
    "print(list(itertools.islice(ng.ngrammer[texts], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO : Cleaned 10000 docs\n",
      "INFO : Cleaned 15000 docs\n",
      "INFO : PROGRESS: at sentence #10000, processed 15787871 words and 1027 word types\n",
      "INFO : Cleaned 20000 docs\n",
      "INFO : Cleaned 25000 docs\n",
      "INFO : PROGRESS: at sentence #20000, processed 39018014 words and 1167 word types\n",
      "INFO : Cleaned 30000 docs\n",
      "INFO : Cleaned 35000 docs\n",
      "INFO : PROGRESS: at sentence #30000, processed 69997670 words and 1236 word types\n",
      "INFO : collected 1236 word types from a corpus of 74443352 words (unigram + bigrams) and 31411 sentences\n",
      "INFO : using 1236 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-189-8eaf941fe9b3>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-189-8eaf941fe9b3>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class StopWordRemover(TransformerMixin):\n",
    "    def __init__(self, docs, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        \n",
    "    def fit(self, texts, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts, *args):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramTransformer(TransformerMixin):\n",
    "    def __init__(n=3, **phrase_kws):\n",
    "        self.n = n\n",
    "        self.phrase_kws = phrase_kws\n",
    "    \n",
    "    def fit(self, texts, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts, *args):\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer that labels tokens in a document with their\n",
    "    part of speech tags.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pos_tags : bool, required\n",
    "        Whether to tag words with their part of speech labels.\n",
    "    lemmatize : bool, required\n",
    "        Whether to lemmatize tokens.\n",
    "    stop_words : book, required\n",
    "        Whether to remove stop words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stop_words, pos_tags=True,\n",
    "                rejoin=True):\n",
    "        self.stop_words = stop_words\n",
    "        self.pos_tags = pos_tags\n",
    "        self.rejoin = rejoin\n",
    "\n",
    "    def tag_pos(self, text):\n",
    "        return [(t, t.pos_) for t in text]\n",
    "\n",
    "    def get_lemmas(self, text):\n",
    "        return [t[0].lemma_ for t in text]\n",
    "\n",
    "    def remove_noise(self, text):\n",
    "        class POSProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer that labels tokens in a document with their\n",
    "    part of speech tags.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pos_tags : bool, required\n",
    "        Whether to tag words with their part of speech labels.\n",
    "    lemmatize : bool, required\n",
    "        Whether to lemmatize tokens.\n",
    "    stop_words : book, required\n",
    "        Whether to remove stop words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stop_words, pos_tags=True,\n",
    "                rejoin=True):\n",
    "        self.stop_words = stop_words\n",
    "        self.pos_tags = pos_tags\n",
    "        self.rejoin = rejoin\n",
    "\n",
    "    def tag_pos(self, text):\n",
    "        return [(t, t.pos_) for t in text]\n",
    "\n",
    "    def get_lemmas(self, text):\n",
    "        return [t[0].lemma_ for t in text]\n",
    "\n",
    "    def remove_noise(self, text):\n",
    "        noise_tags = ['DET', 'NUM', 'SYM']\n",
    "        text = [t for t in text if t[0].text not in self.stop_words]\n",
    "        text = [t for t in text if len(t[0]) > 2]\n",
    "        text = [t for t in text if t[1] not in noise_tags]\n",
    "        text = [t for t in text if ~t[0].like_num]\n",
    "        return text\n",
    "\n",
    "    def join_pos_lemmas(self, pos, lemmas):\n",
    "        return ['{}_{}'.format(l, p[1]).lower() for p, l\n",
    "                in zip(pos, lemmas)]\n",
    "\n",
    "    def fit(self, texts, *args):\n",
    "        return self\n",
    "\n",
    "    def single_string(self, texts):\n",
    "        strings = [' '.join(t) for t in texts]\n",
    "        return strings\n",
    "\n",
    "    def transform(self, texts, *args):\n",
    "        docs = [nlp(sent) for sent in texts]\n",
    "        docs = [self.tag_pos(d) for d in docs]\n",
    "        docs = [self.remove_noise(d) for d in docs]\n",
    "        lemmas = [self.get_lemmas(d) for d in docs]\n",
    "        if self.pos_tags:\n",
    "            docs = [self.join_pos_lemmas(d, l) for d, l\n",
    "                    in zip(docs, lemmas)]\n",
    "        if self.rejoin:\n",
    "            docs = self.single_string(docs)\n",
    "        return docs\n",
    "\n",
    "        text = [t for t in text if t[0].text not in self.stop_words]\n",
    "        text = [t for t in text if len(t[0]) > 2]\n",
    "        text = [t for t in text if t[1] not in noise_tags]\n",
    "        text = [t for t in text if ~t[0].like_num]\n",
    "        return text\n",
    "\n",
    "    def join_pos_lemmas(self, pos, lemmas):\n",
    "        return ['{}_{}'.format(l, p[1]).lower() for p, l\n",
    "                in zip(pos, lemmas)]\n",
    "\n",
    "    def fit(self, texts, *args):\n",
    "        return self\n",
    "\n",
    "    def single_string(self, texts):\n",
    "        strings = [' '.join(t) for t in texts]\n",
    "        return strings\n",
    "\n",
    "    def transform(self, texts, *args):\n",
    "        docs = [nlp(sent) for sent in texts]\n",
    "        docs = [self.tag_pos(d) for d in docs]\n",
    "        docs = [self.remove_noise(d) for d in docs]\n",
    "        lemmas = [self.get_lemmas(d) for d in docs]\n",
    "        if self.pos_tags:\n",
    "            docs = [self.join_pos_lemmas(d, l) for d, l\n",
    "                    in zip(docs, lemmas)]\n",
    "        if self.rejoin:\n",
    "            docs = self.single_string(docs)\n",
    "        return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(corpus)\n",
    "\n",
    "    parsed_descriptions = []\n",
    "    \n",
    "    for doc in nlp.pipe(corpus, batch_size=50, n_threads=-1):\n",
    "        if doc.is_parsed:\n",
    "            doc\n",
    "            tokens.append([n.text for n in doc])\n",
    "            lemma.append([n.lemma_ for n in doc])\n",
    "            pos.append([n.pos_ for n in doc])\n",
    "        else:\n",
    "            # We want to make sure that the lists of parsed results have the\n",
    "            # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "            tokens.append(None)\n",
    "            lemma.append(None)\n",
    "            pos.append(None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
